{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a38b1de3-ba3d-4270-81e9-af7f54b5897e",
   "metadata": {
    "id": "a38b1de3-ba3d-4270-81e9-af7f54b5897e"
   },
   "outputs": [],
   "source": [
    "# !pip install BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f21f967-ba23-447e-abf1-8e740da05e7f",
   "metadata": {
    "id": "3f21f967-ba23-447e-abf1-8e740da05e7f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import zipfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd583c-16cf-41b0-9ad9-7ff4651e30a4",
   "metadata": {
    "id": "23bd583c-16cf-41b0-9ad9-7ff4651e30a4"
   },
   "source": [
    "# Class Explanation: `NewsScraper`\n",
    "\n",
    "## Overview\n",
    "The `NewsScraper` class is designed for scraping news articles from three different Urdu news websites: Geo, Jang, and Express. The class has methods that cater to each site's unique structure and requirements. Below, we will go through the class and its methods, detailing what each function does, the input it takes, and the output it returns.\n",
    "\n",
    "## Class Definition\n",
    "\n",
    "```python\n",
    "class NewsScraper:\n",
    "    def __init__(self, id_=0):\n",
    "        self.id = id_\n",
    "```\n",
    "\n",
    "\n",
    "## Method 1: `get_express_articles`\n",
    "\n",
    "### Description\n",
    "Scrapes news articles from the Express website across categories like saqafat (entertainment), business, sports, science-technology, and world. The method navigates through multiple pages for each category to gather a more extensive dataset.\n",
    "\n",
    "### Input\n",
    "- **`max_pages`**: The number of pages to scrape for each category (default is 7).\n",
    "\n",
    "### Process\n",
    "- Iterates over each category and page.\n",
    "- Requests each category page and finds article cards within `<ul class='tedit-shortnews listing-page'>`.\n",
    "- Extracts the article's headline, link, and content by navigating through `<div class='horiz-news3-caption'>` and `<span class='story-text'>`.\n",
    "\n",
    "### Output\n",
    "- **Returns**: A tuple of:\n",
    "  - A Pandas DataFrame containing columns: `id`, `title`, and `link`).\n",
    "  - A dictionary `express_contents` where the key is the article ID and the value is the article content.\n",
    "\n",
    "### Data Structure\n",
    "- Article cards are identified by `<li>` tags.\n",
    "- Content is structured within `<span class='story-text'>` and `<p>` tags.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8fc81de-6bc7-4bde-92e4-1512dcf43aa0",
   "metadata": {
    "id": "d8fc81de-6bc7-4bde-92e4-1512dcf43aa0"
   },
   "outputs": [],
   "source": [
    "class NewsScraper:\n",
    "    def __init__(self,id_=0):\n",
    "        self.id = id_\n",
    "\n",
    "\n",
    "  # write functions to scrape from other websites\n",
    "    def get_express_articles(self, max_pages=7):\n",
    "        express_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "        }\n",
    "        base_url = 'https://www.express.pk'\n",
    "        categories = ['saqafat', 'business', 'sports', 'science', 'world']   # saqafat is entertainment category\n",
    "\n",
    "        # Iterating over the specified number of pages\n",
    "        for category in categories:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                print(f\"Scraping page {page} of category '{category}'...\")\n",
    "                url = f\"{base_url}/{category}/archives?page={page}\"\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                # Finding article cards\n",
    "                cards = soup.find('ul', class_='tedit-shortnews listing-page').find_all('li')  # Adjust class as per actual site structure\n",
    "                print(f\"\\t--> Found {len(cards)} articles on page {page} of '{category}'.\")\n",
    "\n",
    "                success_count = 0\n",
    "\n",
    "                for card in cards:\n",
    "                    try:\n",
    "                        div = card.find('div',class_='horiz-news3-caption')\n",
    "\n",
    "                        # Article Title\n",
    "                        headline = div.find('a').get_text(strip=True).replace('\\xa0', ' ')\n",
    "\n",
    "                        # Article link\n",
    "                        link = div.find('a')['href']\n",
    "\n",
    "                        # Requesting the content from each article's link\n",
    "                        article_response = requests.get(link)\n",
    "                        article_response.raise_for_status()\n",
    "                        content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "\n",
    "                        # Content arranged in paras inside <span> tags\n",
    "                        paras = content_soup.find('span',class_='story-text').find_all('p')\n",
    "\n",
    "                        combined_text = \" \".join(\n",
    "                        p.get_text(strip=True).replace('\\xa0', ' ').replace('\\u200b', '')\n",
    "                        for p in paras if p.get_text(strip=True)\n",
    "                        )\n",
    "\n",
    "                        # Storing data\n",
    "                        express_df['id'].append(self.id)\n",
    "                        express_df['title'].append(headline)\n",
    "                        express_df['link'].append(link)\n",
    "                        express_df['gold_label'].append(category.replace('saqafat','entertainment').replace('science','science-technology'))\n",
    "                        express_df['content'].append(combined_text)\n",
    "\n",
    "                        # Increment ID and success count\n",
    "                        self.id += 1\n",
    "                        success_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\t--> Failed to scrape an article on page {page} of '{category}': {e}\")\n",
    "\n",
    "                print(f\"\\t--> Successfully scraped {success_count} articles from page {page} of '{category}'.\")\n",
    "            print('')\n",
    "\n",
    "        return pd.DataFrame(express_df)\n",
    "    \n",
    "    \n",
    "    def get_jang_articles(self, max_pages=7):\n",
    "        jang_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "        }\n",
    "        base_url = 'https://jang.com.pk/category'\n",
    "        categories = ['latest-news/entertainment', 'latest-news/business', 'latest-news/sports', 'latest-news/world'] # science and technology is under magazine category and others are under latest-news ('magazine/science-and-technology)\n",
    "\n",
    "        # Iterating over the specified number of pages\n",
    "        for category in categories:\n",
    "            for page in range(1, 2):\n",
    "                print(f\"Scraping page {page} of category '{category}'...\")\n",
    "                url = f\"{base_url}/{category}\"\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                # Finding article cards\n",
    "                cards = soup.find('div', class_='latest_page_right').find_all('li', class_='')\n",
    "                \n",
    "                print(f\"\\t--> Found {len(cards)} articles on page {page} of '{category}'.\")\n",
    "                \n",
    "                success_count = 0\n",
    "                \n",
    "                for card in cards:\n",
    "                    try:\n",
    "                        # Article Title\n",
    "                        a = card.find('div', class_='main-heading').find('a')\n",
    "                        headline = card.find('div', class_='main-heading').find('h2').get_text(strip=True)\n",
    "                        # headline = card.find('h2').get_text(strip=True)\n",
    "                        # Article link\n",
    "                        link = a['href']\n",
    "                        # Requesting the content from each article's link\n",
    "                        article_response = requests.get(link)\n",
    "                        article_response.raise_for_status()\n",
    "                        content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "                        # Content arranged in paras inside <div> tags\n",
    "                        paras = content_soup.find('div', class_='detail_view_content').find_all('p')\n",
    "\n",
    "                        combined_text = \" \".join(\n",
    "                        p.get_text(strip=True).replace('\\xa0', ' ').replace('\\u200b', '')\n",
    "                        for p in paras if p.get_text(strip=True)\n",
    "                        )\n",
    "\n",
    "                        # Storing data\n",
    "                        jang_df['id'].append(self.id)\n",
    "                        jang_df['title'].append(headline)\n",
    "                        jang_df['link'].append(link)\n",
    "                        jang_df['gold_label'].append(category.split('/')[1].replace('science-and-technology','science-technology'))\n",
    "                        jang_df['content'].append(combined_text)\n",
    "\n",
    "                        # Increment ID and success count\n",
    "                        self.id += 1\n",
    "                        success_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\t--> Failed to scrape an article on page {page} of '{category}': {e}\")\n",
    "                        \n",
    "                print(f\"\\t--> Successfully scraped {success_count} articles from page {page} of '{category}'.\")\n",
    "            print('')\n",
    "            \n",
    "        return pd.DataFrame(jang_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b551edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DunyaNewsUrduScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://urdu.dunyanews.tv\"\n",
    "        self.categories = [\"World\", \"Business\", \"Sports\", \"Entertainment\", \"Technology\", \"Cricket\"]\n",
    "        self.articles = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"category\": [],\n",
    "        }\n",
    "        self.article_id = 1\n",
    "        self.box_classes = [\"impcatg newsBox\", \"col-md-6 col-sm-6 col-xs-6\", \"cNewsBox\"]\n",
    "\n",
    "    def get_article_links(self):\n",
    "        for category in self.categories:\n",
    "            section_url = f\"{self.base_url}/index.php/ur/{category}\"\n",
    "            print(f\"Scraping category: {category} -> {section_url}\")\n",
    "\n",
    "            response = requests.get(section_url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            for box_class in self.box_classes:\n",
    "                if box_class == \"impcatg newsBox\":\n",
    "                    boxes = soup.find_all('div', class_=box_class)\n",
    "                    print(f\"Found {len(boxes)} boxes with class '{box_class}'.\")\n",
    "                    for box in boxes:\n",
    "                        try:\n",
    "                            title_tag = box.find('a')\n",
    "                            if title_tag:\n",
    "                                title = title_tag.get_text(strip=True)\n",
    "                                link = title_tag['href']\n",
    "                                full_link = f\"{self.base_url}{link}\"\n",
    "\n",
    "                                content = self.get_article_content(full_link)\n",
    "\n",
    "                                self.articles['id'].append(self.article_id)\n",
    "                                self.articles['title'].append(title)\n",
    "                                self.articles['link'].append(full_link)\n",
    "                                self.articles['content'].append(content)\n",
    "                                \n",
    "                                if (category == \"Cricket\"):\n",
    "                                    self.articles['category'].append(\"Sports\")\n",
    "                                else:\n",
    "                                    self.articles['category'].append(category)\n",
    "\n",
    "                                print(f\"Scraped article {self.article_id}: {title}\")\n",
    "                                self.article_id += 1\n",
    "\n",
    "                                time.sleep(1)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing article in class '{box_class}': {e}\")\n",
    "                        \n",
    "                if box_class == \"col-md-6 col-sm-6 col-xs-6\":\n",
    "                    boxes = soup.find_all('div', class_=box_class)\n",
    "                    print(f\"Found {len(boxes)} boxes with class '{box_class}'.\")\n",
    "                    for box in boxes:\n",
    "                        try:\n",
    "                            title_tag = box.find('a')\n",
    "                            title_tag2 = box.find('h3')\n",
    "                            if title_tag:\n",
    "                                title = title_tag2.get_text(strip=True)\n",
    "                                link = title_tag['href']\n",
    "                                full_link = f\"{self.base_url}{link}\"\n",
    "\n",
    "                                content = self.get_article_content(full_link)\n",
    "\n",
    "                                self.articles['id'].append(self.article_id)\n",
    "                                self.articles['title'].append(title)\n",
    "                                self.articles['link'].append(full_link)\n",
    "                                self.articles['content'].append(content)\n",
    "                                \n",
    "                                if (category == \"Cricket\"):\n",
    "                                    self.articles['category'].append(\"Sports\")\n",
    "                                else:\n",
    "                                    self.articles['category'].append(category)\n",
    "\n",
    "                                print(f\"Scraped article {self.article_id}: {title}\")\n",
    "                                self.article_id += 1\n",
    "\n",
    "                                time.sleep(1)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing article in class '{box_class}': {e}\")\n",
    "                        \n",
    "                if box_class == \"cNewsBox\":\n",
    "                    boxes = soup.find_all('div', class_=box_class)\n",
    "                    print(f\"Found {len(boxes)} boxes with class '{box_class}'.\")\n",
    "                    for box in boxes:\n",
    "                        rows = box.find_all(\"div\", class_=\"col-md-8\")\n",
    "                        print(f\"Found {len(rows)} rows in box '{box_class}'.\")\n",
    "                        try:\n",
    "                            for row in rows:\n",
    "                                try:\n",
    "                                    title_tag = row.find('a')\n",
    "                                    if title_tag:\n",
    "                                        title = title_tag.get_text(strip=True)\n",
    "                                        link = title_tag['href']\n",
    "                                        full_link = f\"{self.base_url}{link}\"\n",
    "\n",
    "                                        content = self.get_article_content(full_link)\n",
    "\n",
    "                                        self.articles['id'].append(self.article_id)\n",
    "                                        self.articles['title'].append(title)\n",
    "                                        self.articles['link'].append(full_link)\n",
    "                                        self.articles['content'].append(content)\n",
    "                                        if (category == \"Cricket\"):\n",
    "                                            self.articles['category'].append(\"Sports\")\n",
    "                                        else:\n",
    "                                            self.articles['category'].append(category)\n",
    "\n",
    "                                        print(f\"Scraped article {self.article_id}: {title}\")\n",
    "                                        self.article_id += 1\n",
    "\n",
    "                                        time.sleep(1)\n",
    "                                except Exception as e:\n",
    "                                    print(f\"Error processing row: {e}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing article in class '{box_class}': {e}\")\n",
    "\n",
    "    def get_article_content(self, link):\n",
    "        response = requests.get(link)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        content_div = soup.find('div', class_='main-news')\n",
    "        if not content_div:\n",
    "            return \"\"\n",
    "\n",
    "        paragraphs = content_div.find_all('p')\n",
    "        content = \" \".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "        return content\n",
    "\n",
    "    def save_to_csv(self, filename=\"./Scrapped Data/dunya_urdu_articles.csv\"):\n",
    "        df = pd.DataFrame(self.articles)\n",
    "        df.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"Articles saved to {filename}\")\n",
    "\n",
    "scraper = DunyaNewsUrduScraper()\n",
    "scraper.get_article_links()\n",
    "scraper.save_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b2d88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ARYNewsUrduScraper:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://urdu.arynews.tv/category\"\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0'\n",
    "        }\n",
    "        # self.categories = [\"World\", \"Business\", \"Sports\", \"Entertainment\", \"Technology\", \"Cricket\"]\n",
    "        self.categories = [\"sports-2/\", \"کاروباری-خبریں/\", \"fun-o-sakafat/\", \"سائنس-اور-ٹیکنالوجی/\", \"international-2/\"]\n",
    "        self.categories_2 = {\"sports-2/\":\"Sports\", \"کاروباری-خبریں/\":\"Business\", \"fun-o-sakafat/\":\"Entertainment\", \"سائنس-اور-ٹیکنالوجی/\":\"Technology\", \"international-2/\":\"World\"}\n",
    "        self.articles = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"category\": [],\n",
    "        }\n",
    "        self.article_id = 1\n",
    "        self.box_classes = [\"col-md-6 col-sm-6 col-xs-6\", \"cNewsBox\"]\n",
    "\n",
    "    def get_article_links(self):\n",
    "        for category in self.categories:\n",
    "            section_url = f\"{self.base_url}/{category}\"\n",
    "            # print(section_url)\n",
    "            print(f\"Scraping category: {category} -> {section_url}\")\n",
    "\n",
    "            response = requests.get(section_url, headers = self.headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            \n",
    "            boxes = soup.find_all('div', id=\"tdi_84\")\n",
    "            # print(f\"Found {len(boxes)} boxes with id tdi_84.\")\n",
    "            \n",
    "            for box in boxes:\n",
    "                try:\n",
    "                    title_tag = box.find('a')\n",
    "                    if title_tag:\n",
    "                        title = title_tag['title']\n",
    "                        full_link = title_tag['href']\n",
    "\n",
    "                        content = self.get_article_content(full_link)\n",
    "\n",
    "                        self.articles['id'].append(self.article_id)\n",
    "                        self.articles['title'].append(title)\n",
    "                        self.articles['link'].append(full_link)\n",
    "                        self.articles['content'].append(content)\n",
    "                        \n",
    "                        if (category == \"Cricket\"):\n",
    "                            self.articles['category'].append(\"Sports\")\n",
    "                        else:\n",
    "                            self.articles['category'].append(self.categories_2[category])\n",
    "\n",
    "                        print(f\"Scraped article {self.article_id}: {title}\")\n",
    "                        self.article_id += 1\n",
    "\n",
    "                        time.sleep(1)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing article in class': {e}\")\n",
    "                    \n",
    "            page_number = 1\n",
    "            while page_number <= 15:\n",
    "                print(f\"Scraping page {page_number}...\")\n",
    "\n",
    "                page_url = f\"{section_url}page/{page_number}\"\n",
    "                \n",
    "                response = requests.get(page_url, headers=self.headers)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                \n",
    "                boxes = soup.find_all('div', id=\"tdi_85\")\n",
    "                # print(f\"Found {len(boxes)} boxes with id tdi_85.\")\n",
    "                \n",
    "                # tdb_module_loop td_module_wrap td-animation-stack td-cpt-post\n",
    "                \n",
    "                for box in boxes:\n",
    "                    sub_boxes = soup.find_all('div', class_=\"tdb_module_loop td_module_wrap td-animation-stack td-cpt-post\")\n",
    "                    # print(f\"Found {len(boxes)} boxes with class\")\n",
    "                    for sub_box in sub_boxes:\n",
    "                        try:\n",
    "                            title_tag = sub_box.find('a')\n",
    "                            if title_tag:\n",
    "                                title = title_tag['title']\n",
    "                                full_link = title_tag['href']\n",
    "\n",
    "                                content = self.get_article_content(full_link)\n",
    "\n",
    "                                self.articles['id'].append(self.article_id)\n",
    "                                self.articles['title'].append(title)\n",
    "                                self.articles['link'].append(full_link)\n",
    "                                self.articles['content'].append(content)\n",
    "                                \n",
    "                                if (category == \"Cricket\"):\n",
    "                                    self.articles['category'].append(\"Sports\")\n",
    "                                else:\n",
    "                                    self.articles['category'].append(self.categories_2[category])\n",
    "\n",
    "                                print(f\"Scraped article {self.article_id}: {title}\")\n",
    "                                self.article_id += 1\n",
    "\n",
    "                                time.sleep(1)\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing article in class': {e}\")\n",
    "                            \n",
    "                page_number += 1\n",
    "\n",
    "                    \n",
    "    def get_article_content(self, link):\n",
    "        response = requests.get(link, headers=self.headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # content_div = soup.find('div', class_='main-news')\n",
    "        content_box = soup.find('div', {'data-td-block-uid': 'tdi_102'})\n",
    "        if not content_box:\n",
    "            return \"\"\n",
    "\n",
    "        paragraphs = content_box.find_all('p')\n",
    "\n",
    "        content = []\n",
    "        for p in paragraphs:\n",
    "            if p.find('img'):\n",
    "                continue\n",
    "\n",
    "            text = p.get_text(strip=True)\n",
    "            if text:\n",
    "                content.append(text)\n",
    "\n",
    "        return \" \".join(content)\n",
    "\n",
    "    def save_to_csv(self, filename=\"./Scrapped Data/ary_urdu_articles.csv\"):\n",
    "        df = pd.DataFrame(self.articles)\n",
    "        df.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"Articles saved to {filename}\")\n",
    "\n",
    "scraper = ARYNewsUrduScraper()\n",
    "scraper.get_article_links()\n",
    "scraper.save_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a8ad94-10b0-4458-bb7f-3402eecd80d1",
   "metadata": {
    "id": "e9a8ad94-10b0-4458-bb7f-3402eecd80d1"
   },
   "outputs": [],
   "source": [
    "scraper = NewsScraper() # Dunya and Ary Scrapers were run already by a group member hence not running again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "321373e7-8ef4-468f-81d0-8be61fe2ba85",
   "metadata": {
    "id": "321373e7-8ef4-468f-81d0-8be61fe2ba85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 1 of 'saqafat'.\n",
      "\t--> Failed to scrape an article on page 1 of 'saqafat': 404 Client Error: Not Found for url: https://www.express.pk/story/2732336/dosti-international-festival-starts-in-lahore\n",
      "\t--> Successfully scraped 9 articles from page 1 of 'saqafat'.\n",
      "Scraping page 2 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 2 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'saqafat'.\n",
      "Scraping page 3 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 3 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'saqafat'.\n",
      "Scraping page 4 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 4 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'saqafat'.\n",
      "Scraping page 5 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 5 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'saqafat'.\n",
      "Scraping page 6 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 6 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'saqafat'.\n",
      "Scraping page 7 of category 'saqafat'...\n",
      "\t--> Found 10 articles on page 7 of 'saqafat'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'saqafat'.\n",
      "\n",
      "Scraping page 1 of category 'business'...\n",
      "\t--> Found 10 articles on page 1 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'business'.\n",
      "Scraping page 2 of category 'business'...\n",
      "\t--> Found 10 articles on page 2 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'business'.\n",
      "Scraping page 3 of category 'business'...\n",
      "\t--> Found 10 articles on page 3 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'business'.\n",
      "Scraping page 4 of category 'business'...\n",
      "\t--> Found 10 articles on page 4 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'business'.\n",
      "Scraping page 5 of category 'business'...\n",
      "\t--> Found 10 articles on page 5 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'business'.\n",
      "Scraping page 6 of category 'business'...\n",
      "\t--> Found 10 articles on page 6 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'business'.\n",
      "Scraping page 7 of category 'business'...\n",
      "\t--> Found 10 articles on page 7 of 'business'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'business'.\n",
      "\n",
      "Scraping page 1 of category 'sports'...\n",
      "\t--> Found 10 articles on page 1 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'sports'.\n",
      "Scraping page 2 of category 'sports'...\n",
      "\t--> Found 10 articles on page 2 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'sports'.\n",
      "Scraping page 3 of category 'sports'...\n",
      "\t--> Found 10 articles on page 3 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'sports'.\n",
      "Scraping page 4 of category 'sports'...\n",
      "\t--> Found 10 articles on page 4 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'sports'.\n",
      "Scraping page 5 of category 'sports'...\n",
      "\t--> Found 10 articles on page 5 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'sports'.\n",
      "Scraping page 6 of category 'sports'...\n",
      "\t--> Found 10 articles on page 6 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'sports'.\n",
      "Scraping page 7 of category 'sports'...\n",
      "\t--> Found 10 articles on page 7 of 'sports'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'sports'.\n",
      "\n",
      "Scraping page 1 of category 'science'...\n",
      "\t--> Found 10 articles on page 1 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'science'.\n",
      "Scraping page 2 of category 'science'...\n",
      "\t--> Found 10 articles on page 2 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'science'.\n",
      "Scraping page 3 of category 'science'...\n",
      "\t--> Found 10 articles on page 3 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'science'.\n",
      "Scraping page 4 of category 'science'...\n",
      "\t--> Found 10 articles on page 4 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'science'.\n",
      "Scraping page 5 of category 'science'...\n",
      "\t--> Found 10 articles on page 5 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'science'.\n",
      "Scraping page 6 of category 'science'...\n",
      "\t--> Found 10 articles on page 6 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'science'.\n",
      "Scraping page 7 of category 'science'...\n",
      "\t--> Found 10 articles on page 7 of 'science'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'science'.\n",
      "\n",
      "Scraping page 1 of category 'world'...\n",
      "\t--> Found 10 articles on page 1 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 1 of 'world'.\n",
      "Scraping page 2 of category 'world'...\n",
      "\t--> Found 10 articles on page 2 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 2 of 'world'.\n",
      "Scraping page 3 of category 'world'...\n",
      "\t--> Found 10 articles on page 3 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 3 of 'world'.\n",
      "Scraping page 4 of category 'world'...\n",
      "\t--> Found 10 articles on page 4 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 4 of 'world'.\n",
      "Scraping page 5 of category 'world'...\n",
      "\t--> Found 10 articles on page 5 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 5 of 'world'.\n",
      "Scraping page 6 of category 'world'...\n",
      "\t--> Found 10 articles on page 6 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 6 of 'world'.\n",
      "Scraping page 7 of category 'world'...\n",
      "\t--> Found 10 articles on page 7 of 'world'.\n",
      "\t--> Successfully scraped 10 articles from page 7 of 'world'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "express_df = scraper.get_express_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "789ee75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 of category 'latest-news/entertainment'...\n",
      "\t--> Found 99 articles on page 1 of 'latest-news/entertainment'.\n",
      "\t--> Successfully scraped 99 articles from page 1 of 'latest-news/entertainment'.\n",
      "\n",
      "Scraping page 1 of category 'latest-news/business'...\n",
      "\t--> Found 97 articles on page 1 of 'latest-news/business'.\n",
      "\t--> Successfully scraped 97 articles from page 1 of 'latest-news/business'.\n",
      "\n",
      "Scraping page 1 of category 'latest-news/sports'...\n",
      "\t--> Found 99 articles on page 1 of 'latest-news/sports'.\n",
      "\t--> Successfully scraped 99 articles from page 1 of 'latest-news/sports'.\n",
      "\n",
      "Scraping page 1 of category 'latest-news/world'...\n",
      "\t--> Found 100 articles on page 1 of 'latest-news/world'.\n",
      "\t--> Successfully scraped 100 articles from page 1 of 'latest-news/world'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jang_df = scraper.get_jang_articles()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nn7TyroayZhg",
   "metadata": {
    "id": "nn7TyroayZhg"
   },
   "source": [
    "# Output\n",
    "- Save a combined csv of all 3 sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668040f6-1f3b-4400-8daa-39b1296a151e",
   "metadata": {
    "id": "668040f6-1f3b-4400-8daa-39b1296a151e"
   },
   "outputs": [],
   "source": [
    "# saving jang data to a csv file\n",
    "jang_df.to_csv('./Scrapped Data/jang_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bd2760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving express data to a csv file\n",
    "express_df.to_csv('./Scrapped Data/express_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
